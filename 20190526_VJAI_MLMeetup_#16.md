# VJAI Meetup #16
:clock3: Thời gian: 14h~18h Chủ nhật - 26/05/2019 (mở cửa đón khách 13h40 ~ 13h55)

:point_right: Form đăng ký tham dự: https://forms.gle/DRiZg5bVyJKtQZuo8
(Hạn cuối đăng ký là 23:59 thứ 3 ngày 21/5/2019)

Sau buổi meetup với chủ đề ứng dụng AI trong y tế và giao lưu cùng VinAI, VJAI meetup trong tháng 5 sẽ đến với 2 chủ đề liên quan đến Deep Learning: 1 chủ đề về kỹ thuật quan trọng trong việc huấn luyện mô hình và 1 chủ đề về ứng dụng Deep Learning trong edge computing.

* Thời gian: 14h~18h Chủ nhật - 26/5/2018 (mở cửa đón khách 13h40 ~ 13h55)
* Địa điểm: Microsoft Japan office tầng 30 <br>
108-0075 東京都港区港南 2-16-3 品川グランドセントラルタワー <br>
https://www.microsoft.com/ja-jp/mscorp/branch/sgt.aspx

Hướng dẫn: từ ga JR Shinagawa cửa Kounan (港南口) đi theo skywalker là đến chỗ reception (tầng 2). Tại đây bạn sẽ lấy vé vào và lên tầng 30.

Chú ý: do yêu cầu an ninh toà nhà, đề nghị các bạn điền thông tin theo form dưới đây. Hạn cuối đăng ký là **23:59 thứ 3 - 21/5/2019**.

https://forms.gle/DRiZg5bVyJKtQZuo8

Ai không kịp đăng ký trước thời hạn trên sẽ không được vào toà nhà nên ngoài ấn Going thì các bạn **nhớ điền vào form** trên nhé.

Lần này ban tổ chức đã sắp xếp phòng rộng hơn nhưng cũng chỉ chứa được 50 người nên nếu số người đăng ký nhiều hơn, ban tổ chức sẽ áp dụng hình thức Lottery (như đã thông báo trên facebook group). Thêm nữa là việc vào toà nhà không dễ dàng nên các bạn tính toán thời gian cẩn thận để đến đúng giờ nhé.

Hẹn gặp lại cả nhà vào buổi sắp tới!

Nội dung chương trình:

---
### Lê Trung Kiên
AI Software Engineer, Microsoft Japan

### Regularization in Deep Learning
A simple model cannot learn from the problem, whereas a complex one can learn too well and overfit the training dataset. Both cases produce a model that does not generalize well. A practical approach is to start with a larger model then apply regularization to counter overfit issue in order to produce the final solution. The talk will cover the fundermental explanation about overfitting and why regularization is an effective solution. Several key regularization techniques such as dropout, batchnorm, etc will be discussed. At the end, let's check our knowledge by applying some of those techniques in a real world problem.

---
### Ngô Huy Cừ


### Deep learning at the edge with FPGA and ASIC: Trends and Challenges
Deep Neural Networks (DNNs) achieve state-of-the-art results in many fields such as image classification, speech recognition, language processing…. Many issues like low latency, communication to the cloud, privacy could be solved by implementing DNN computing at the edge. However, most of DNN algorithms used today are very costly in terms of energy consumption and processing time because of their large amount of required computations and large model sizes. Therefore, DNN inference at the edge remains challenging today. This talk will review some recent advanced techniques and design solutions on Field Programmable Gate Array (FPGA) and Application Specific Integrated Circuit (ASIC) such as quantization, weight reduction, activation function approximation, in memory computation to minimize processing time and power consumption of edge deep learning systems.
