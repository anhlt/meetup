# VN Machine Learning Meetup #13
Form đăng ký tham dự: https://goo.gl/forms/pjMCFs7gDayq7sW93

Chào cả nhà, đến với buổi Meetup cuối cùng của năm 2018, chúng ta sẽ đến với 2 bài toán nổi trội trong lĩnh vực xử lý ngôn ngữ tự nhiên là dịch máy (Machine Translattion) và đọc hiểu văn bản (Reading Comprehension). Hãy cùng đến xem xem máy tính đang tiến sát với con người trong việc xử lý ngôn ngữ đến đâu nhé.

* Thời gian: 14h~18h Chủ nhật - 16/12/2018 (mở cửa đón khách 13h40 ~ 13h55)
* Địa điểm: BizReach, Inc. 渋谷363清水ビル<br>
〒150-0002, 3 Chome-6-3 Shibuya, Tokyo
https://www.google.com/maps/place/%E6%B8%8B%E8%B0%B7363%E6%B8%85%E6%B0%B4%E3%83%93%E3%83%AB/@35.6579054,139.7029824,17z/data=!4m12!1m6!3m5!1s0x60188b596a766fc5:0xec3765750fe79ab1!2z5riL6LC3MzYz5riF5rC044OT44Or!8m2!3d35.6579054!4d139.7051711!3m4!1s0x60188b596a766fc5:0xec3765750fe79ab1!8m2!3d35.6579054!4d139.7051711

Hạn cuối đăng ký là **23:59 thứ 3 - 11/12/2018**.

https://goo.gl/forms/pjMCFs7gDayq7sW93

Ai không kịp đăng ký trước thời hạn trên sẽ không được vào toà nhà nên ngoài ấn Going thì các bạn **nhớ điền vào form** trên nhé.

Lần này ban tổ chức đã sắp xếp phòng rộng hơn nhưng cũng chỉ chứa được 50 người nên nếu số người đăng ký nhiều hơn, ban tổ chức sẽ áp dụng hình thức Lottery (như đã thông báo trên facebook group). Thêm nữa là việc vào toà nhà không dễ dàng nên các bạn tính toán thời gian cẩn thận để đến đúng giờ nhé. (TBC)

Hẹn gặp lại cả nhà vào buổi sắp tới!

Nội dung chương trình:
---
### Phạm Quang Khang
AI Solutions Architect

### Pay more attention to Attention mechanism
Since proposed in 2017, Attention has been evolved and become one of the most attended mechanism in fields like computer vision, nature language processing and even in recommendation system. Taking machine translation problem as an example, this talk will walk through the intuition of this mechanism as well as its evolution from encode-decoder attention to self-attention and lastly, Transformer (Attention is all you need: rank 4th most popular paper of all time on Arxiv Sanity), which removed completely conventional RNNs architecture.

### References
1. Bahdanau et al,. Neural Machine Translation by jointly learning to align and translate
2. Thang Luong et al,. Effective Approaches to Attention-based Neural Machine Translation
3. Lin et al,. A structured self-attentive sentence embedding Vaswani et al,.
4. Multilple authors, Attention is all you need
---

### Nguyễn Phước Tất Đạt
AI Research Engineer (BizReach, Inc.)

### Machine can do reading comprehension test for you
Reading comprehension, or question answering, is one of the most important and challenging natural language processing (NLP) tasks, in which a concrete answer is produced with respect to a question on an input text. This task is hard for even human when the input texts are complex, and it is used to test language proficiency of readers.

With the advancement of Deep Learning (DL) in many domains, NLP researchers are seeking for various ways to apply DL techniques to NLP tasks, including reading comprehension task. In this trend, a robust deep neural network architecture is designed to capture hierarchical representations of language, which then maps a question and a context as inputs directly to the appropriate answer as output.

In this talk, we will explore some neural network architectures for reading comprehension task which give state-of-the-art results even better than human performance. At the end of the talk, let's give machine an input text, then you ask it answers.

### References:
1. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
2. A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi, and Q. V. Le, "Qanet: Combining local convolution with global self-attention for reading comprehension," arXiv preprint arXiv:1804.09541, 2018.
3. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv:1606.05250.
